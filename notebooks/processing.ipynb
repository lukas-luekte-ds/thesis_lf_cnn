{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffbe7e8d",
   "metadata": {},
   "source": [
    "# Processing\n",
    "The preprocessing relies on the PSML dataset, which provides a dataloader\n",
    "## The dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "747270f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw data points (3 years in Caizo zone 1): 1573923 (1573923, 12) 36.43255943149464\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "raw_path = \"/media/lll/T9/PSML/Minute-level Load and Renewable/CAISO_zone_1_.csv\"\n",
    "raw = pd.read_csv(raw_path)\n",
    "\n",
    "print(\"raw data points (3 years in Caizo zone 1):\", len(raw), raw.shape, len(raw)/43201)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94b32507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/67\n",
      "1/67\n",
      "2/67\n",
      "3/67\n",
      "4/67\n",
      "5/67\n",
      "6/67\n",
      "7/67\n",
      "8/67\n",
      "9/67\n",
      "10/67\n",
      "11/67\n",
      "12/67\n",
      "13/67\n",
      "14/67\n",
      "15/67\n",
      "16/67\n",
      "17/67\n",
      "18/67\n",
      "19/67\n",
      "20/67\n",
      "21/67\n",
      "22/67\n",
      "23/67\n",
      "24/67\n",
      "25/67\n",
      "26/67\n",
      "27/67\n",
      "28/67\n",
      "29/67\n",
      "30/67\n",
      "31/67\n",
      "32/67\n",
      "33/67\n",
      "34/67\n",
      "35/67\n",
      "36/67\n",
      "37/67\n",
      "38/67\n",
      "39/67\n",
      "40/67\n",
      "41/67\n",
      "42/67\n",
      "43/67\n",
      "44/67\n",
      "45/67\n",
      "46/67\n",
      "47/67\n",
      "48/67\n",
      "49/67\n",
      "50/67\n",
      "51/67\n",
      "52/67\n",
      "53/67\n",
      "54/67\n",
      "55/67\n",
      "56/67\n",
      "57/67\n",
      "58/67\n",
      "59/67\n",
      "60/67\n",
      "61/67\n",
      "62/67\n",
      "63/67\n",
      "64/67\n",
      "65/67\n",
      "66/67\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/lll/thesis_lf_cnn/baselines/benchmark/Code\")\n",
    "\n",
    "from dataloader import TimeSeriesLoader\n",
    "\n",
    "# just dont reload the kernel \n",
    "loader = TimeSeriesLoader(task = 'forecasting', root = \"/media/lll/T9/PSML\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dae2dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = loader.load(batch_size=512, shuffle=False) # no shuffle for time series (maybe this is not correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d64db77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the train data:  3720\n",
      "length of the test data:  43201\n"
     ]
    }
   ],
   "source": [
    "print(\"length of the train data: \", len(train_loader.dataset))\n",
    "print(\"length of the test data: \", len(test_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "934e1102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "075b0034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(batch): 3\n",
      "batch[0] shape: torch.Size([512, 1331])\n",
      "batch[1] shape: torch.Size([512, 6])\n",
      "batch[2] shape: torch.Size([512, 6])\n",
      "if following tensor contains only 0 the mask is not needed: \n",
      "tensor([0, 0, 0, 0, 0, 0])\n",
      "len(batch): 3\n",
      "batch[0] shape: torch.Size([512, 1331])\n",
      "batch[1] shape: torch.Size([512, 6])\n",
      "batch[2] shape: torch.Size([512, 6])\n",
      "if following tensor contains only 0 the mask is not needed: \n",
      "tensor([0, 0, 0, 0, 0, 0])\n",
      "len(batch): 3\n",
      "batch[0] shape: torch.Size([512, 1331])\n",
      "batch[1] shape: torch.Size([512, 6])\n",
      "batch[2] shape: torch.Size([512, 6])\n",
      "if following tensor contains only 0 the mask is not needed: \n",
      "tensor([0, 0, 0, 0, 0, 0])\n",
      "len(batch): 3\n",
      "batch[0] shape: torch.Size([512, 1331])\n",
      "batch[1] shape: torch.Size([512, 6])\n",
      "batch[2] shape: torch.Size([512, 6])\n",
      "if following tensor contains only 0 the mask is not needed: \n",
      "tensor([0, 0, 0, 0, 0, 0])\n",
      "len(batch): 3\n",
      "batch[0] shape: torch.Size([512, 1331])\n",
      "batch[1] shape: torch.Size([512, 6])\n",
      "batch[2] shape: torch.Size([512, 6])\n",
      "if following tensor contains only 0 the mask is not needed: \n",
      "tensor([0, 0, 0, 0, 0, 0])\n",
      "len(batch): 3\n",
      "batch[0] shape: torch.Size([512, 1331])\n",
      "batch[1] shape: torch.Size([512, 6])\n",
      "batch[2] shape: torch.Size([512, 6])\n",
      "if following tensor contains only 0 the mask is not needed: \n",
      "tensor([0, 0, 0, 0, 0, 0])\n",
      "len(batch): 3\n",
      "batch[0] shape: torch.Size([512, 1331])\n",
      "batch[1] shape: torch.Size([512, 6])\n",
      "batch[2] shape: torch.Size([512, 6])\n",
      "if following tensor contains only 0 the mask is not needed: \n",
      "tensor([0, 0, 0, 0, 0, 0])\n",
      "len(batch): 3\n",
      "batch[0] shape: torch.Size([136, 1331])\n",
      "batch[1] shape: torch.Size([136, 6])\n",
      "batch[2] shape: torch.Size([136, 6])\n",
      "if following tensor contains only 0 the mask is not needed: \n",
      "tensor([0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(\"len(batch):\", len(batch))\n",
    "    for i, item in enumerate(batch):\n",
    "        print(f\"batch[{i}] shape:\", item.shape)\n",
    "    print(\"if following tensor contains only 0 the mask is not needed: \")\n",
    "    print(sum(batch[2]==0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29adcf68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training batches:  8\n",
      "Number of testing batches:  85\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training batches: \", len(train_loader))\n",
    "print(\"Number of testing batches: \", len(test_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8573719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fensterl채nge: 1 Features pro Schritt: 1331\n",
      "Fensterl채nge: 11 Features pro Schritt: 121\n",
      "Fensterl채nge: 121 Features pro Schritt: 11\n",
      "tensor([ 1.0785e+00,  1.1828e-01,  6.3931e-01,  6.6000e+01,  9.5500e+02,\n",
      "         5.8600e+02, -1.2400e+01,  5.6980e+01,  3.6000e+00,  8.4900e+00,\n",
      "         2.2900e+01,  1.0786e+00,  1.2091e-01,  6.4057e-01,  6.6000e+01,\n",
      "         9.5560e+02,  5.8780e+02, -1.2400e+01,  5.6888e+01,  3.6000e+00,\n",
      "         8.4800e+00,  2.2920e+01,  1.0787e+00,  1.2354e-01,  6.4183e-01,\n",
      "         6.6000e+01,  9.5620e+02,  5.8960e+02, -1.2400e+01,  5.6796e+01,\n",
      "         3.6000e+00,  8.4700e+00,  2.2940e+01,  1.0788e+00,  1.2617e-01,\n",
      "         6.4309e-01,  6.6000e+01,  9.5680e+02,  5.9140e+02, -1.2400e+01,\n",
      "         5.6704e+01,  3.6000e+00,  8.4600e+00,  2.2960e+01,  1.0789e+00,\n",
      "         1.2880e-01,  6.4435e-01,  6.6000e+01,  9.5740e+02,  5.9320e+02])\n",
      "tensor([ 6.6360e+02, -1.2800e+01,  5.2616e+01,  3.0000e+00,  7.1320e+00,\n",
      "         2.5220e+01,  1.0870e+00,  6.1336e-02,  6.9286e-01,  6.9000e+01,\n",
      "         9.8060e+02,  6.6320e+02, -1.2800e+01,  5.2642e+01,  3.0000e+00,\n",
      "         7.1240e+00,  2.5240e+01,  1.0870e+00,  6.2004e-02,  6.9240e-01,\n",
      "         6.9000e+01,  9.8040e+02,  6.6280e+02, -1.2800e+01,  5.2668e+01,\n",
      "         3.0000e+00,  7.1160e+00,  2.5260e+01,  1.0871e+00,  6.2673e-02,\n",
      "         6.9194e-01,  6.9000e+01,  9.8020e+02,  6.6240e+02, -1.2800e+01,\n",
      "         5.2694e+01,  3.0000e+00,  7.1080e+00,  2.5280e+01,  1.0871e+00,\n",
      "         6.3342e-02,  6.9147e-01,  6.9000e+01,  9.8000e+02,  6.6200e+02,\n",
      "        -1.2800e+01,  5.2720e+01,  3.0000e+00,  7.1000e+00,  2.5300e+01])\n"
     ]
    }
   ],
   "source": [
    "feature_dim = batch[0].shape[1]\n",
    "\n",
    "for T in range(1, 200):\n",
    "    if feature_dim % T == 0:\n",
    "        print(\"Fensterl채nge:\", T, \"Features pro Schritt:\", feature_dim // T)\n",
    "\n",
    "\n",
    "batch[0].shape  \n",
    "print(batch[0][0][:50])    \n",
    "print(batch[0][0][-50:])    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31573dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ID', 'month_day', 'weekday', 'holiday', 'DHI', 'DNI', 'GHI',\n",
      "       'Dew Point', 'Solar Zenith Angle', 'Wind Speed', 'Relative Humidity',\n",
      "       'Temperature', 'yl_t', 'yl_t+60(val)', 'yl_t+60(flag)',\n",
      "       'yl_t+1440(val)', 'yl_t+1440(flag)', 'yw_t', 'yw_t+5(val)',\n",
      "       'yw_t+5(flag)', 'yw_t+30(val)', 'yw_t+30(flag)', 'ys_t', 'ys_t+5(val)',\n",
      "       'ys_t+5(flag)', 'ys_t+30(val)', 'ys_t+30(flag)', 'train_flag'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "## get column names \n",
    "import pandas as pd\n",
    "cols = pd.read_csv(\"/media/lll/T9/PSML/processed_dataset/forecasting/CAISO_zone_1_2018.csv\", nrows = 0)\n",
    "print(cols.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32706dc",
   "metadata": {},
   "source": [
    "## Trying to understand where I lose most of my training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3ee48d",
   "metadata": {},
   "source": [
    "### What does the loading of the processed dataset do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e16cad55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data before sliding window: 480960\n",
      "train data after sliding window: 480840\n",
      "training data before flags matrix: 480720\n",
      "training data after flags matrix (selected_index): 3720\n",
      "training data after first flags filter only (only_one_index): 8011\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"/home/lll/thesis_lf_cnn/baselines/benchmark/Code\")\n",
    "\n",
    "data = pd.read_csv(\"/media/lll/T9/PSML/processed_dataset/forecasting/CAISO_zone_1_2018.csv\")\n",
    "\n",
    "train_flag = data['train_flag'].to_numpy()\n",
    "all_train_idx = np.sort(np.argwhere(train_flag == 1).reshape([-1]))\n",
    "print(\"training data before sliding window:\", len(all_train_idx))\n",
    "\n",
    "sliding_window = 120\n",
    "training_index = all_train_idx[sliding_window:]\n",
    "print(\"train data after sliding window:\", len(training_index))\n",
    "\n",
    "from BenchmarkModel.LoadForecasting.processing import task_prediction_horizon, external_feature_names\n",
    "\n",
    "history_column_names = []\n",
    "target_val_column_names = []\n",
    "for task_name, task_prediction_horizon_list in task_prediction_horizon.items():\n",
    "    history_column_names.append(f'y{task_name[0]}_t')\n",
    "    for horizon_val in task_prediction_horizon_list:\n",
    "        target_val_column_names.append(f'y{task_name[0]}_t+{horizon_val}(val)')\n",
    "\n",
    "target_flag_column_names = [c.replace(\"val\", \"flag\") for c in target_val_column_names]\n",
    "\n",
    "training_validation_target_flag = data[target_flag_column_names].to_numpy()[training_index[sliding_window:]]\n",
    "print(\"training data before flags matrix:\", training_validation_target_flag.shape[0])\n",
    "\n",
    " # We use all flag filters at the same time which results in losing a big chunk of data\n",
    " # Maybe it would be smart to solve the different Horizons one by one and set specialized flag for each one to train each on max data\n",
    "selected_index = np.argwhere(np.prod(training_validation_target_flag, axis=-1) == 1).reshape([-1])\n",
    "print(\"training data after flags matrix (selected_index):\", len(selected_index))\n",
    "\n",
    "only_one_index = np.argwhere(training_validation_target_flag[:, 0] == 1).reshape([-1])\n",
    "print(\"training data after first flags filter only (only_one_index):\", len(only_one_index))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "717dc64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all training data: 480720\n",
      "number of targets: 6\n",
      "After every flag filter\n",
      "yl_t+60(flag)                       :   8011 / 480720\n",
      "yl_t+1440(flag)                     :   7988 / 480720\n",
      "yw_t+5(flag)                        : 480641 / 480720\n",
      "yw_t+30(flag)                       : 480604 / 480720\n",
      "ys_t+5(flag)                        : 234252 / 480720\n",
      "ys_t+30(flag)                       : 225919 / 480720\n",
      "datapoints left for all filters combined\n",
      "datapoints:   3720 / 480720\n"
     ]
    }
   ],
   "source": [
    "istory_column_names = []\n",
    "target_val_column_names = []\n",
    "\n",
    "for task_name, horizon_list in task_prediction_horizon.items():\n",
    "    history_column_names.append(f\"y{task_name[0]}_t\")\n",
    "    for h in horizon_list:\n",
    "        target_val_column_names.append(f\"y{task_name[0]}_t+{h}(val)\")\n",
    "\n",
    "target_flag_column_names = [c.replace(\"(val)\", \"(flag)\") for c in target_val_column_names]\n",
    "\n",
    "\n",
    "training_validation_target_flag = data[target_flag_column_names].to_numpy()[training_index[sliding_window:]]\n",
    "\n",
    "num_samples = training_validation_target_flag.shape[0]\n",
    "\n",
    "print(\"all training data:\", num_samples)\n",
    "print(\"number of targets:\", training_validation_target_flag.shape[1])\n",
    "print(\"After every flag filter\")\n",
    "\n",
    "result = []\n",
    "\n",
    "for i, col in enumerate(target_flag_column_names):\n",
    "    valid_mask = (training_validation_target_flag[:, i] == 1)\n",
    "    valid_count = valid_mask.sum()\n",
    "    result.append((col, valid_count))\n",
    "    print(f\"{col:35s} : {valid_count:6d} / {num_samples}\")\n",
    "\n",
    "# combined filter\n",
    "\n",
    "all_valid = np.prod(training_validation_target_flag, axis=-1) == 1\n",
    "print(\"datapoints left for all filters combined\")\n",
    "print(f\"datapoints: {all_valid.sum():6d} / {num_samples}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
