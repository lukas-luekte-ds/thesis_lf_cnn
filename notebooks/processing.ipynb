{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffbe7e8d",
   "metadata": {},
   "source": [
    "# Processing\n",
    "The preprocessing relies on the PSML dataset, which provides a dataloader\n",
    "## The dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "747270f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw data points (3 years in Caizo zone 1): 1573923 (1573923, 12) 36.43255943149464\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "raw_path = \"/media/lll/T9/PSML/Minute-level Load and Renewable/CAISO_zone_1_.csv\"\n",
    "raw = pd.read_csv(raw_path)\n",
    "\n",
    "print(\"raw data points (3 years in Caizo zone 1):\", len(raw), raw.shape, len(raw)/43201)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94b32507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/67\n",
      "1/67\n",
      "2/67\n",
      "3/67\n",
      "4/67\n",
      "5/67\n",
      "6/67\n",
      "7/67\n",
      "8/67\n",
      "9/67\n",
      "10/67\n",
      "11/67\n",
      "12/67\n",
      "13/67\n",
      "14/67\n",
      "15/67\n",
      "16/67\n",
      "17/67\n",
      "18/67\n",
      "19/67\n",
      "20/67\n",
      "21/67\n",
      "22/67\n",
      "23/67\n",
      "24/67\n",
      "25/67\n",
      "26/67\n",
      "27/67\n",
      "28/67\n",
      "29/67\n",
      "30/67\n",
      "31/67\n",
      "32/67\n",
      "33/67\n",
      "34/67\n",
      "35/67\n",
      "36/67\n",
      "37/67\n",
      "38/67\n",
      "39/67\n",
      "40/67\n",
      "41/67\n",
      "42/67\n",
      "43/67\n",
      "44/67\n",
      "45/67\n",
      "46/67\n",
      "47/67\n",
      "48/67\n",
      "49/67\n",
      "50/67\n",
      "51/67\n",
      "52/67\n",
      "53/67\n",
      "54/67\n",
      "55/67\n",
      "56/67\n",
      "57/67\n",
      "58/67\n",
      "59/67\n",
      "60/67\n",
      "61/67\n",
      "62/67\n",
      "63/67\n",
      "64/67\n",
      "65/67\n",
      "66/67\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/lll/thesis_lf_cnn/baselines/benchmark/Code\")\n",
    "\n",
    "from dataloader import TimeSeriesLoader\n",
    "\n",
    "# just dont reload the kernel \n",
    "loader = TimeSeriesLoader(task = 'forecasting', root = \"/media/lll/T9/PSML\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dae2dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = loader.load(batch_size=512, shuffle=False) # no shuffle for time series (maybe this is not correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d64db77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the train data:  3720\n",
      "length of the test data:  43201\n"
     ]
    }
   ],
   "source": [
    "print(\"length of the train data: \", len(train_loader.dataset))\n",
    "print(\"length of the test data: \", len(test_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "075b0034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(batch): 3\n",
      "batch[0] shape: torch.Size([512, 1331])\n",
      "batch[1] shape: torch.Size([512, 6])\n",
      "batch[2] shape: torch.Size([512, 6])\n",
      "if following tensor contains only 0 the mask is not needed: \n",
      "tensor([0, 0, 0, 0, 0, 0])\n",
      "len(batch): 3\n",
      "batch[0] shape: torch.Size([512, 1331])\n",
      "batch[1] shape: torch.Size([512, 6])\n",
      "batch[2] shape: torch.Size([512, 6])\n",
      "if following tensor contains only 0 the mask is not needed: \n",
      "tensor([0, 0, 0, 0, 0, 0])\n",
      "len(batch): 3\n",
      "batch[0] shape: torch.Size([512, 1331])\n",
      "batch[1] shape: torch.Size([512, 6])\n",
      "batch[2] shape: torch.Size([512, 6])\n",
      "if following tensor contains only 0 the mask is not needed: \n",
      "tensor([0, 0, 0, 0, 0, 0])\n",
      "len(batch): 3\n",
      "batch[0] shape: torch.Size([512, 1331])\n",
      "batch[1] shape: torch.Size([512, 6])\n",
      "batch[2] shape: torch.Size([512, 6])\n",
      "if following tensor contains only 0 the mask is not needed: \n",
      "tensor([0, 0, 0, 0, 0, 0])\n",
      "len(batch): 3\n",
      "batch[0] shape: torch.Size([512, 1331])\n",
      "batch[1] shape: torch.Size([512, 6])\n",
      "batch[2] shape: torch.Size([512, 6])\n",
      "if following tensor contains only 0 the mask is not needed: \n",
      "tensor([0, 0, 0, 0, 0, 0])\n",
      "len(batch): 3\n",
      "batch[0] shape: torch.Size([512, 1331])\n",
      "batch[1] shape: torch.Size([512, 6])\n",
      "batch[2] shape: torch.Size([512, 6])\n",
      "if following tensor contains only 0 the mask is not needed: \n",
      "tensor([0, 0, 0, 0, 0, 0])\n",
      "len(batch): 3\n",
      "batch[0] shape: torch.Size([512, 1331])\n",
      "batch[1] shape: torch.Size([512, 6])\n",
      "batch[2] shape: torch.Size([512, 6])\n",
      "if following tensor contains only 0 the mask is not needed: \n",
      "tensor([0, 0, 0, 0, 0, 0])\n",
      "len(batch): 3\n",
      "batch[0] shape: torch.Size([136, 1331])\n",
      "batch[1] shape: torch.Size([136, 6])\n",
      "batch[2] shape: torch.Size([136, 6])\n",
      "if following tensor contains only 0 the mask is not needed: \n",
      "tensor([0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(\"len(batch):\", len(batch))\n",
    "    for i, item in enumerate(batch):\n",
    "        print(f\"batch[{i}] shape:\", item.shape)\n",
    "    print(\"if following tensor contains only 0 the mask is not needed: \")\n",
    "    print(sum(batch[2]==0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29adcf68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training batches:  8\n",
      "Number of testing batches:  85\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training batches: \", len(train_loader))\n",
    "print(\"Number of testing batches: \", len(test_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8573719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fensterlänge: 1 Features pro Schritt: 1331\n",
      "Fensterlänge: 11 Features pro Schritt: 121\n",
      "Fensterlänge: 121 Features pro Schritt: 11\n",
      "tensor([ 7.1042e-01,  9.1583e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -1.0400e+01,  1.2556e+02,  1.5000e+00,  2.2260e+01,\n",
      "         1.0300e+01,  7.1074e-01,  8.9282e-03,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00, -1.0400e+01,  1.2535e+02,  1.5000e+00,\n",
      "         2.2260e+01,  1.0300e+01,  7.1106e-01,  8.6980e-03,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0400e+01,  1.2515e+02,\n",
      "         1.5000e+00,  2.2260e+01,  1.0300e+01,  7.1138e-01,  8.4678e-03,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0400e+01,\n",
      "         1.2494e+02,  1.5000e+00,  2.2260e+01,  1.0300e+01,  7.1170e-01,\n",
      "         8.2376e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00])\n",
      "tensor([ 0.0000e+00, -1.1100e+01,  1.0217e+02,  1.5000e+00,  2.2354e+01,\n",
      "         9.3800e+00,  7.4415e-01,  8.5332e-03,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00, -1.1100e+01,  1.0197e+02,  1.5000e+00,\n",
      "         2.2388e+01,  9.3600e+00,  7.4441e-01,  8.9081e-03,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00, -1.1100e+01,  1.0178e+02,\n",
      "         1.5000e+00,  2.2422e+01,  9.3400e+00,  7.4466e-01,  9.2830e-03,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.1100e+01,\n",
      "         1.0158e+02,  1.5000e+00,  2.2456e+01,  9.3200e+00,  7.4492e-01,\n",
      "         9.6579e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "        -1.1100e+01,  1.0139e+02,  1.5000e+00,  2.2490e+01,  9.3000e+00])\n"
     ]
    }
   ],
   "source": [
    "feature_dim = batch[0].shape[1]\n",
    "\n",
    "for T in range(1, 200):\n",
    "    if feature_dim % T == 0:\n",
    "        print(\"Fensterlänge:\", T, \"Features pro Schritt:\", feature_dim // T)\n",
    "\n",
    "\n",
    "batch[0].shape  \n",
    "print(batch[0][0][:50])    \n",
    "print(batch[0][0][-50:])    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31573dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ID', 'month_day', 'weekday', 'holiday', 'DHI', 'DNI', 'GHI',\n",
      "       'Dew Point', 'Solar Zenith Angle', 'Wind Speed', 'Relative Humidity',\n",
      "       'Temperature', 'yl_t', 'yl_t+60(val)', 'yl_t+60(flag)',\n",
      "       'yl_t+1440(val)', 'yl_t+1440(flag)', 'yw_t', 'yw_t+5(val)',\n",
      "       'yw_t+5(flag)', 'yw_t+30(val)', 'yw_t+30(flag)', 'ys_t', 'ys_t+5(val)',\n",
      "       'ys_t+5(flag)', 'ys_t+30(val)', 'ys_t+30(flag)', 'train_flag'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "## get column names \n",
    "import pandas as pd\n",
    "cols = pd.read_csv(\"/media/lll/T9/PSML/processed_dataset/forecasting/CAISO_zone_1_2018.csv\", nrows = 0)\n",
    "print(cols.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32706dc",
   "metadata": {},
   "source": [
    "## Trying to understand where I lose most of my training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3ee48d",
   "metadata": {},
   "source": [
    "### What does the loading of the processed dataset do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e16cad55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainpunkte vor Sliding Window: 480960\n",
      "Trainpunkte nach Sliding Window: 480840\n",
      "Trainpunkte nach Flags-Matrix: 480720\n",
      "Trainpunkte nach Flag-Filter (selected_index): 3720\n",
      "Trainpunkte nach nur dem ersten Flag-Filter (only_one_index): 8011\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"/home/lll/thesis_lf_cnn/baselines/benchmark/Code\")\n",
    "\n",
    "data = pd.read_csv(\"/media/lll/T9/PSML/processed_dataset/forecasting/CAISO_zone_1_2018.csv\")\n",
    "\n",
    "train_flag = data['train_flag'].to_numpy()\n",
    "all_train_idx = np.sort(np.argwhere(train_flag == 1).reshape([-1]))\n",
    "print(\"Trainpunkte vor Sliding Window:\", len(all_train_idx))\n",
    "\n",
    "sliding_window = 120\n",
    "training_index = all_train_idx[sliding_window:]\n",
    "print(\"Trainpunkte nach Sliding Window:\", len(training_index))\n",
    "\n",
    "# Flags vorbereiten\n",
    "from BenchmarkModel.LoadForecasting.processing import task_prediction_horizon, external_feature_names\n",
    "\n",
    "history_column_names = []\n",
    "target_val_column_names = []\n",
    "for task_name, task_prediction_horizon_list in task_prediction_horizon.items():\n",
    "    history_column_names.append(f'y{task_name[0]}_t')\n",
    "    for horizon_val in task_prediction_horizon_list:\n",
    "        target_val_column_names.append(f'y{task_name[0]}_t+{horizon_val}(val)')\n",
    "\n",
    "target_flag_column_names = [c.replace(\"val\", \"flag\") for c in target_val_column_names]\n",
    "\n",
    "training_validation_target_flag = data[target_flag_column_names].to_numpy()[training_index[sliding_window:]]\n",
    "print(\"Trainpunkte nach Flags-Matrix:\", training_validation_target_flag.shape[0])\n",
    "\n",
    " # We use all flag filters at the same time which results in losing a big chunk of data\n",
    " # Maybe it would be smart to solve the different Horizons one by one and set specialized flag for each one to train each on max data\n",
    "selected_index = np.argwhere(np.prod(training_validation_target_flag, axis=-1) == 1).reshape([-1])\n",
    "print(\"Trainpunkte nach Flag-Filter (selected_index):\", len(selected_index))\n",
    "\n",
    "only_one_index = np.argwhere(training_validation_target_flag[:, 0] == 1).reshape([-1])\n",
    "print(\"Trainpunkte nach nur dem ersten Flag-Filter (only_one_index):\", len(only_one_index))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717dc64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gesamtzahl möglicher Trainingspunkte: 480720\n",
      "Anzahl Horizonte: 6\n",
      "\n",
      "=== Gültige Samples je Horizont (Einzel-Filter) ===\n",
      "\n",
      "yl_t+60(flag)                       :   8011 / 480720\n",
      "yl_t+1440(flag)                     :   7988 / 480720\n",
      "yw_t+5(flag)                        : 480641 / 480720\n",
      "yw_t+30(flag)                       : 480604 / 480720\n",
      "ys_t+5(flag)                        : 234252 / 480720\n",
      "ys_t+30(flag)                       : 225919 / 480720\n",
      "\n",
      "=== Gültige Samples, wenn ALLE Horizonte gleichzeitig gültig sind ===\n",
      "Alle Horizonte gültig    :   3720 / 480720\n"
     ]
    }
   ],
   "source": [
    "history_column_names = []\n",
    "target_val_column_names = []\n",
    "\n",
    "for task_name, horizon_list in task_prediction_horizon.items():\n",
    "    history_column_names.append(f\"y{task_name[0]}_t\")\n",
    "    for h in horizon_list:\n",
    "        target_val_column_names.append(f\"y{task_name[0]}_t+{h}(val)\")\n",
    "\n",
    "target_flag_column_names = [c.replace(\"(val)\", \"(flag)\") for c in target_val_column_names]\n",
    "\n",
    "# -------------------------------------------\n",
    "# 2. Flags als numpy-array holen\n",
    "# -------------------------------------------\n",
    "\n",
    "training_validation_target_flag = data[target_flag_column_names].to_numpy()[training_index[sliding_window:]]\n",
    "\n",
    "num_samples = training_validation_target_flag.shape[0]\n",
    "\n",
    "print(\"Gesamtzahl möglicher Trainingspunkte:\", num_samples)\n",
    "print(\"Anzahl Horizonte:\", training_validation_target_flag.shape[1])\n",
    "print(\"\\n=== Gültige Samples je Horizont (Einzel-Filter) ===\\n\")\n",
    "\n",
    "# -------------------------------------------\n",
    "# 3. Gültige Samples je Horizont ausgeben\n",
    "# -------------------------------------------\n",
    "\n",
    "result = []\n",
    "\n",
    "for i, col in enumerate(target_flag_column_names):\n",
    "    valid_mask = (training_validation_target_flag[:, i] == 1)\n",
    "    valid_count = valid_mask.sum()\n",
    "    result.append((col, valid_count))\n",
    "    print(f\"{col:35s} : {valid_count:6d} / {num_samples}\")\n",
    "\n",
    "# -------------------------------------------\n",
    "# 4. Zusätzlich: wie viele bleiben übrig, wenn ALLE Horizonte gültig sein müssen?\n",
    "# -------------------------------------------\n",
    "\n",
    "all_valid = np.prod(training_validation_target_flag, axis=-1) == 1\n",
    "print(\"\\n=== Gültige Samples, wenn ALLE Horizonte gleichzeitig gültig sind ===\")\n",
    "print(f\"Alle Horizonte gültig    : {all_valid.sum():6d} / {num_samples}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
