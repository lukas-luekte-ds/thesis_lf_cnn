{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9793bab9",
   "metadata": {},
   "source": [
    "# Here I wanna experiment with the training of a simple mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829947c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mlp'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdataloader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TimeSeriesLoader\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmlp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m STLFMLP\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'mlp'"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import argparse\n",
    "import os\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from utils import fix_seed, get_device, ensure_dir, save_json, git_hash\n",
    "import sys\n",
    "from dataloader import TimeSeriesLoader\n",
    "sys.path.append(\"/home/lll/thesis_lf_cnn/src/models\")\n",
    "from mlp import STLFMLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84d8f17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe this can be somewhere else\n",
    "@torch.no_grad()\n",
    "def eval_mae_per_target(model, loader, device):\n",
    "    import numpy as np\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    for x, y, _ in loader:\n",
    "        x = x.to(device).float() \n",
    "        y = y.to(device).float()\n",
    "        preds.append(model(x).cpu().numpy())\n",
    "        trues.append(y.cpu().numpy())\n",
    "    P = np.concatenate(preds, 0)\n",
    "    Y = np.concatenate(trues, 0)\n",
    "    per_target = np.mean(np.abs(P - Y), axis=0)  # shape (6,)\n",
    "    return per_target.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09130fe",
   "metadata": {},
   "source": [
    "# the main training loop (maybe I can structure this cleaner later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dd9108c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train loop\n",
    "def train_loop(model, train_loader, val_loader, device, cfg, ckpt_dir):\n",
    "    opt_name = cfg[\"train\"][\"optimizer\"].lower()\n",
    "    if opt_name == \"adamw\":\n",
    "        opt = torch.optim.AdamW(model.parameters(), lr=cfg[\"train\"][\"lr\"],\n",
    "                                weight_decay=cfg[\"train\"][\"weight_decay\"])\n",
    "    else:\n",
    "        opt = torch.optim.Adam(model.parameters(), lr=cfg[\"train\"][\"lr\"])\n",
    "\n",
    "    patience = cfg[\"train\"][\"early_stopping_patience\"]\n",
    "    best_val = float(\"inf\")\n",
    "    wait = 0\n",
    "    history = {\"train_loss\": [], \"val_mae\": []}\n",
    "    os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "    for epoch in range(1, cfg[\"train\"][\"epochs\"]+1):\n",
    "        model.train()\n",
    "        batch_losses = []\n",
    "        for x, y, flag in train_loader:\n",
    "            x = x.to(device).float()\n",
    "            y = y.to(device).float()\n",
    "            pred = model(x)\n",
    "            lt = cfg[\"train\"][\"loss_type\"]\n",
    "            if lt == \"mae\":\n",
    "                loss = (pred - y).abs().mean()\n",
    "            elif lt == \"mse\":\n",
    "                loss = torch.nn.functional.mse_loss(pred, y)\n",
    "            else:\n",
    "                loss = torch.nn.functional.huber_loss(pred, y, delta=cfg[\"train\"][\"huber_delta\"])\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            # optional: torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "            batch_losses.append(loss.item())\n",
    "\n",
    "        train_loss = float(np.mean(batch_losses))\n",
    "        val_mae = eval_mae(model, val_loader, device)\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_mae\"].append(val_mae)\n",
    "\n",
    "        improved = val_mae + 1e-9 < best_val\n",
    "        if improved:\n",
    "            best_val = val_mae\n",
    "            wait = 0\n",
    "            torch.save({\"model\": model.state_dict()}, os.path.join(ckpt_dir, \"best-val.pt\"))\n",
    "        else:\n",
    "            wait += 1\n",
    "\n",
    "        print(f\"Epoch {epoch:03d} | train_loss {train_loss:.4f} | val_MAE {val_mae:.4f} | best {best_val:.4f} | wait {wait}\")\n",
    "        if patience and wait >= patience:\n",
    "            print(\"Early stopping.\")\n",
    "            break\n",
    "\n",
    "    return best_val, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a50b087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded config: mlp_stlf_baseline\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'TimeSeriesLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 131\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    130\u001b[39m     args = parse_args()\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(cfg_path)\u001b[39m\n\u001b[32m     26\u001b[39m device = get_device(cfg[\u001b[33m\"\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# trying to use the dataloader as it is used in the benchmark for comparability (implement another dataloader later for performance)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m loader = \u001b[43mTimeSeriesLoader\u001b[49m(task=\u001b[33m'\u001b[39m\u001b[33mforecasting\u001b[39m\u001b[33m'\u001b[39m, root=cfg[\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mroot\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# train loader full will be used as it is but we need an evaluation set and therefore split the test set\u001b[39;00m\n\u001b[32m     32\u001b[39m train_loader_full, test_loader = loader.load(\n\u001b[32m     33\u001b[39m     batch_size=cfg[\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mbatch_size\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     34\u001b[39m     shuffle=cfg[\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mshuffle\u001b[39m\u001b[33m\"\u001b[39m]  \n\u001b[32m     35\u001b[39m )  \n",
      "\u001b[31mNameError\u001b[39m: name 'TimeSeriesLoader' is not defined"
     ]
    }
   ],
   "source": [
    "# put this to utils or somewhere else, since loading configs will be used in every model\n",
    "def load_config(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "# this should be in eval.py or utils.py later and maybe implement different eval strategies and encapsulate them as eval(mae, ...)\n",
    "@torch.no_grad()\n",
    "def eval_mae(model, loader, device):\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    for x, y, flag in loader:   \n",
    "        x = x.to(device).float()\n",
    "        y = y.to(device).float()\n",
    "        p = model(x)\n",
    "        preds.append(p.cpu().numpy())\n",
    "        trues.append(y.cpu().numpy())\n",
    "    P = np.concatenate(preds, 0)\n",
    "    Y = np.concatenate(trues, 0)\n",
    "    return float(np.mean(np.abs(P - Y)))\n",
    "\n",
    "# this is the real training loop\n",
    "def main(cfg_path):\n",
    "    cfg = load_config(cfg_path)\n",
    "    print(\"Loaded config:\", cfg[\"exp_name\"])\n",
    "    fix_seed(cfg[\"seed\"], deterministic=True) # we want to use a fixed seed and a deterministic algorithm to ensure reproducibility\n",
    "    device = get_device(cfg[\"device\"])\n",
    "\n",
    "    # trying to use the dataloader as it is used in the benchmark for comparability (implement another dataloader later for performance)\n",
    "    loader = TimeSeriesLoader(task='forecasting', root=cfg[\"data\"][\"root\"])\n",
    "\n",
    "    # train loader full will be used as it is but we need an evaluation set and therefore split the test set\n",
    "    train_loader_full, test_loader = loader.load(\n",
    "        batch_size=cfg[\"data\"][\"batch_size\"],\n",
    "        shuffle=cfg[\"data\"][\"shuffle\"]  \n",
    "    )  \n",
    "\n",
    "    # split a val from training set (makeing sure it is chronological)\n",
    "    full_ds = train_loader_full.dataset\n",
    "    N = len(full_ds)\n",
    "    val_len = int(N * cfg[\"data\"][\"val_ratio\"])\n",
    "    train_len = N - val_len\n",
    "    train_idx = list(range(0, train_len))\n",
    "    val_idx   = list(range(train_len, N))\n",
    "\n",
    "    # optional : num_workers or pin_memory in config to increase training (only use if enough RAM and training is too slow)\n",
    "    train_loader = DataLoader(Subset(full_ds, train_idx),\n",
    "                              batch_size=cfg[\"data\"][\"batch_size\"],\n",
    "                              shuffle=cfg[\"data\"][\"shuffle\"],\n",
    "                              drop_last=False)\n",
    "    val_loader   = DataLoader(Subset(full_ds, val_idx),\n",
    "                              batch_size=cfg[\"data\"][\"batch_size\"],\n",
    "                              shuffle=False,\n",
    "                              drop_last=False)\n",
    "\n",
    "    # the model architecture\n",
    "    model = STLFMLP(\n",
    "        input_dim=cfg[\"model\"][\"input_dim\"],\n",
    "        hidden_sizes=cfg[\"model\"][\"hidden_sizes\"],\n",
    "        out_dim=cfg[\"model\"][\"out_dim\"],\n",
    "        dropout=cfg[\"model\"][\"dropout\"],\n",
    "        use_layernorm=cfg[\"model\"][\"use_layernorm\"],\n",
    "        use_input_norm=cfg[\"model\"][\"use_input_norm\"],\n",
    "        activation=cfg[\"model\"][\"activation\"],\n",
    "    ).to(device)\n",
    "\n",
    "    # using this only for training data to improve training but NEVER on val or test, input normalizer active is prob always the best option so I remains active\n",
    "    model.fit_input_normalizer_from_loader(train_loader)\n",
    "\n",
    "    # training loop and checkpoint to save the best model\n",
    "    exp_dir  = os.path.join(cfg[\"out\"][\"dir\"], cfg[\"exp_name\"])\n",
    "    ckpt_dir = os.path.join(exp_dir, \"checkpoints\")\n",
    "    ensure_dir(exp_dir)\n",
    "    best_val, history = train_loop(model, train_loader, val_loader, device, cfg, ckpt_dir)\n",
    "\n",
    "    # laod the best model and not the last one\n",
    "    ckpt = torch.load(os.path.join(ckpt_dir, \"best-val.pt\"), map_location=device)\n",
    "    model.load_state_dict(ckpt[\"model\"])\n",
    "\n",
    "    model.eval()\n",
    "    ids_all, preds_all = [], []\n",
    "    with torch.no_grad():\n",
    "        for ID, x in test_loader:\n",
    "            x = x.to(device).float()\n",
    "            p = model(x).cpu().numpy()\n",
    "            ids_all.append(ID.cpu().numpy())\n",
    "            preds_all.append(p)\n",
    "\n",
    "    IDs = np.concatenate(ids_all, 0)\n",
    "    P   = np.concatenate(preds_all, 0)\n",
    "\n",
    "    cols = [\"yl_t+60\",\"yl_t+1440\",\"yw_t+5\",\"yw_t+30\",\"ys_t+5\",\"ys_t+30\"]\n",
    "    preds_df = pd.DataFrame(P, columns=cols)\n",
    "    preds_df.insert(0, \"ID\", IDs)\n",
    "    preds_df.to_csv(os.path.join(exp_dir, \"preds.csv\"), index=False)\n",
    "    print(\"saved test predictions:\", os.path.join(exp_dir, \"preds.csv\"))\n",
    "\n",
    "    # collect meta data\n",
    "    meta = {\n",
    "        \"git_hash\": git_hash(),\n",
    "        \"config\": cfg,\n",
    "        \"best_val_mae\": best_val,\n",
    "        \"n_train\": len(train_idx),\n",
    "        \"n_val\": len(val_idx),\n",
    "        \"history\": history,\n",
    "    }\n",
    "    save_json(os.path.join(exp_dir, \"metrics.json\"), meta)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(history[\"train_loss\"], label=\"train_loss\")\n",
    "    plt.plot(history[\"val_mae\"], label=\"val_mae\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(exp_dir, \"learning_curve.png\"))\n",
    "\n",
    "    per_target = eval_mae_per_target(model, val_loader, device)\n",
    "    print(\"Val-MAE per target:\", per_target)\n",
    "\n",
    "    print(\"done. artifacts in:\", exp_dir)\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--config\", type=str, default=\"../configs/mlp.yaml\")\n",
    "    # use Jupyter args:\n",
    "    known, _ = ap.parse_known_args()\n",
    "    return known\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    main(args.config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
